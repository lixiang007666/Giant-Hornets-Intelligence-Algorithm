# -*- coding: utf-8 -*-
"""bee_image_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G4ZkIVYfpw5eh2nRM6TzukZtM7WsZgvB

#### Copyright 2018 Google LLC.
"""

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""# Bee vs. Hornet Image Classification using InceptionV3

This code is adapted from an exercise that looked at two techniques for repurposing feature data generated from image models that have already been trained on large sets of data, **feature extraction** and **fine tuning**. We will use this adaptation to improve the accuracy of our bee vs. hornet classification model.
See the orignal notebook here:
https://colab.research.google.com/github/google/eng-edu/blob/master/ml/pc/exercises/image_classification_part3.ipynb?hl=en#scrollTo=YHK6DyunSbs4

## Feature Extraction Using a Pretrained Model

One thing that is commonly done in computer vision is to take a model trained on a very large dataset, run it on your own, smaller dataset, and extract the intermediate representations (features) that the model generates. These representations are frequently informative for your own computer vision task, even though the task may be quite different from the problem that the original model was trained on. This versatility and repurposability of convnets is one of the most interesting aspects of deep learning.

In our case, we will use the [Inception V3 model](https://arxiv.org/abs/1512.00567) developed at Google, and pre-trained on [ImageNet](http://image-net.org/), a large dataset of web images (1.4M images and 1000 classes). This is a powerful model; let's see what the features that it has learned can do for our bee vs. hornet problem.

First, we need to pick which intermediate layer of Inception V3 we will use for feature extraction. A common practice is to use the output of the very last layer before the `Flatten` operation, the so-called "bottleneck layer." The reasoning here is that the following fully connected layers will be too specialized for the task the network was trained on, and thus the features learned by these layers won't be very useful for a new task. The bottleneck features, however, retain much generality.

Let's instantiate an Inception V3 model preloaded with weights trained on ImageNet:
"""

import os

from tensorflow.keras import layers
from tensorflow.keras import Model

"""Now let's download the weights:"""

# !wget --no-check-certificate \
#     https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \
#     -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5

from tensorflow.keras.applications.inception_v3 import InceptionV3

local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'
pre_trained_model = InceptionV3(
    input_shape=(150, 150, 3), include_top=False, weights=None)
pre_trained_model.load_weights(local_weights_file)

"""By specifying the `include_top=False` argument, we load a network that doesn't include the classification layers at the topâ€”ideal for feature extraction.

Let's make the model non-trainable, since we will only use it for feature extraction; we won't update the weights of the pretrained model during training.
"""

for layer in pre_trained_model.layers:
  layer.trainable = False

"""The layer we will use for feature extraction in Inception v3 is called `mixed7`. It is not the bottleneck of the network, but we are using it to keep a sufficiently large feature map (7x7 in this case). (Using the bottleneck layer would have resulting in a 3x3 feature map, which is a bit small.) Let's get the output from `mixed7`:"""

last_layer = pre_trained_model.get_layer('mixed7')
print('last layer output shape:', last_layer.output_shape)
last_output = last_layer.output

"""Now let's stick a fully connected classifier on top of `last_output`:"""

from tensorflow.keras.optimizers import RMSprop

# Flatten the output layer to 1 dimension
x = layers.Flatten()(last_output)
# Add a fully connected layer with 1,024 hidden units and ReLU activation
x = layers.Dense(1024, activation='relu')(x)
# Add a dropout rate of 0.2
x = layers.Dropout(0.2)(x)
# Add a final sigmoid layer for classification
x = layers.Dense(1, activation='sigmoid')(x)

# Configure and compile the model
model = Model(pre_trained_model.input, x)
model.compile(loss='binary_crossentropy',
              optimizer=RMSprop(lr=0.0001),
              metrics=['acc'])

"""#Data Preprocessing

Let's set up data generators that will read pictures in our source folders, convert them to float32 tensors, and feed them (with their labels) to our network. We'll have one generator for the training images and one for the validation images. Our generators will yield batches of 20 images of size 150x150 and their labels (binary).

As you may already know, data that goes into neural networks should usually be normalized in some way to make it more amenable to processing by the network. (It is uncommon to feed raw pixels into a convnet.) In our case, we will preprocess our images by normalizing the pixel values to be in the [0, 1] range (originally all values are in the [0, 255] range).

In Keras this can be done via the keras.preprocessing.image.ImageDataGenerator class using the rescale parameter. This ImageDataGenerator class allows you to instantiate generators of augmented image batches (and their labels) via .flow(data, labels) or .flow_from_directory(directory). These generators can then be used with the Keras model methods that accept data generators as inputs: fit_generator, evaluate_generator, and predict_generator.

##Training

Let's train on 80% images available, for 15 epochs, and validate on all 564 images. (This may take a few minutes to run.)
"""

from google.colab import drive
drive.mount("/content/drive")

import os
import zipfile

from tensorflow.keras.preprocessing.image import ImageDataGenerator


# Below are two methods to acess the images in the zip file and update the base directory
#local_zip = '/content/drive/My Drive/tmp/BeeClassificationData.zip'
#!unzip -u local_zip -d '/content/drive/My Drive/BeeClassificationData'

#zip_ref = zipfile.ZipFile(local_zip, 'r')
#zip_ref.extractall('/tmp')
#zip_ref.close()

# Define our example directories and files
base_dir = '/content/drive/My Drive/Data/BeeClassificationData'
train_dir = os.path.join(base_dir, 'train')
validation_dir = os.path.join(base_dir, 'validation')

# Directory with our training bee pictures
train_bees_dir = os.path.join(train_dir, 'bees')

# Directory with our training hornet pictures
train_hornets_dir = os.path.join(train_dir, 'hornets')

# Directory with our validation bee pictures
validation_bees_dir = os.path.join(validation_dir, 'bees')

# Directory with our validation hornet pictures
validation_hornets_dir = os.path.join(validation_dir, 'hornets')

train_bees_fnames = os.listdir(train_bees_dir)
train_hornets_fnames = os.listdir(train_hornets_dir)

# Add our data-augmentation parameters to ImageDataGenerator
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True)

# Note that the validation data should not be augmented!
val_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
        train_dir, # This is the source directory for training images
        target_size=(150, 150),  # All images will be resized to 150x150
        batch_size=20,
        # Since we use binary_crossentropy loss, we need binary labels
        class_mode='binary')

# Flow validation images in batches of 20 using val_datagen generator
validation_generator = val_datagen.flow_from_directory(
        validation_dir,
        target_size=(150, 150),
        batch_size=20,
        class_mode='binary')

"""Finally, let's train the model using the features we extracted. We'll train on all 2256 images available, and validate on all 564 validation images. The training will stop if the validation loss is minimized before the last epoch is reached."""

from keras.callbacks import EarlyStopping
es = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1)

history = model.fit_generator(
      train_generator,
      steps_per_epoch=70,
      epochs=20,
      validation_data=validation_generator,
      validation_steps=17, callbacks=[es])

model.save(base_dir + 'inceptionmodel')

"""##Evaluating Accuracy and Loss for the Model

Let's plot the training/validation accuracy and loss as collected during training:
"""

import matplotlib.pyplot as plt

# Retrieve a list of accuracy results on training and validation data
# sets for each training epoch
acc = history.history['acc']
val_acc = history.history['val_acc']

# Retrieve a list of list results on training and validation data
# sets for each training epoch
loss = history.history['loss']
val_loss = history.history['val_loss']

# Get number of epochs
epochs = range(len(acc))

# Plot training and validation accuracy per epoch
plt.plot(epochs, acc, label="Training Data")
plt.plot(epochs, val_acc, label="Validation Data")
plt.legend(loc="upper right")
plt.ylabel('Accuracy')
plt.xlabel('# of Epochs')
plt.title('Training and Validation Accuracy')

plt.figure()

# Plot training and validation loss per epoch
plt.plot(epochs, loss, label="Training Data")
plt.plot(epochs, val_loss, label="Validation Data")
plt.legend(loc="upper right")
plt.ylabel('Loss')
plt.xlabel('# of Epochs')
plt.title('Training and Validation Loss')

"""## Further Improving Accuracy with Fine-Tuning

In our feature-extraction experiment, we only tried adding two classification layers on top of an Inception V3 layer. The weights of the pretrained network were not updated during training. One way to increase performance even further is to "fine-tune" the weights of the top layers of the pretrained model alongside the training of the top-level classifier. A couple of important notes on fine-tuning:

- **Fine-tuning should only be attempted *after* you have trained the top-level classifier with the pretrained model set to non-trainable**. If you add a randomly initialized classifier on top of a pretrained model and attempt to train all layers jointly, the magnitude of the gradient updates will be too large (due to the random weights from the classifier), and your pretrained model will just forget everything it has learned.
- Additionally, we **fine-tune only the *top layers* of the pre-trained model** rather than all layers of the pretrained model because, in a convnet, the higher up a layer is, the more specialized it is. The first few layers in a convnet learn very simple and generic features, which generalize to almost all types of images. But as you go higher up, the features are increasingly specific to the dataset that the model is trained on. The goal of fine-tuning is to adapt these specialized features to work with the new dataset.

All we need to do to implement fine-tuning is to set the top layers of Inception V3 to be trainable, recompile the model (necessary for these changes to take effect), and resume training. Let's unfreeze all layers belonging to the `mixed7` moduleâ€”i.e., all layers found after `mixed6`â€”and recompile the model:
"""

from tensorflow.keras.optimizers import SGD

unfreeze = False

# Unfreeze all models after "mixed6"
for layer in pre_trained_model.layers:
  if unfreeze:
    layer.trainable = True
  if layer.name == 'mixed6':
    unfreeze = True

# As an optimizer, here we will use SGD 
# with a very low learning rate (0.00001)
model.compile(loss='binary_crossentropy',
              optimizer=SGD(
                  lr=0.00001, 
                  momentum=0.9),
              metrics=['acc'])

"""Now let's retrain the model. We'll train on all 2256 images available, for 20 epochs, and validate on all validation images."""

history = model.fit_generator(
      train_generator,
      steps_per_epoch=100,
      epochs=50,
      validation_data=validation_generator,
      validation_steps=50,
      verbose=2)

"""Let's plot the training and validation loss and accuracy to show it conclusively:"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import matplotlib.pyplot as plt

# Retrieve a list of accuracy results on training and validation data
# sets for each training epoch
acc = history.history['acc']
val_acc = history.history['val_acc']

# Retrieve a list of list results on training and validation data
# sets for each training epoch
loss = history.history['loss']
val_loss = history.history['val_loss']

# Get number of epochs
epochs = range(len(acc))

# Plot training and validation accuracy per epoch
plt.plot(epochs, loss, label="Training Data")
plt.plot(epochs, val_loss, label="Validation Data")
plt.legend(loc="upper right")
plt.ylabel('Accuracy')
plt.xlabel('# of Epochs')
plt.title('Training and Validation Accuracy')

plt.figure()

# Plot training and validation loss per epoch
plt.plot(epochs, loss, label="Training Data")
plt.plot(epochs, val_loss, label="Validation Data")
plt.legend(loc="upper right")
plt.ylabel('Loss')
plt.xlabel('# of Epochs')
plt.title('Training and Validation Loss')

"""## Clean Up

Run the following cell to terminate the kernel and free memory resources:
"""

import os, signal
os.kill(os.getpid(), signal.SIGKILL)